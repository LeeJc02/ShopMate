#!/usr/bin/env python3
"""
æ€§èƒ½å‹æµ‹è„šæœ¬

è¯„æµ‹æŒ‡æ ‡ï¼š
- QPS (Queries Per Second): æ¯ç§’å¤„ç†è¯·æ±‚æ•°
- Latency P50/P95/P99: å“åº”æ—¶é—´åˆ†ä½æ•°
- æˆåŠŸç‡: è¯·æ±‚æˆåŠŸæ¯”ä¾‹
- ååé‡: å•ä½æ—¶é—´å¤„ç†çš„æ€»è¯·æ±‚æ•°
"""

import json
import time
import asyncio
import statistics
from pathlib import Path
from datetime import datetime
from typing import NamedTuple
from concurrent.futures import ThreadPoolExecutor

import aiohttp


class BenchmarkConfig(NamedTuple):
    """å‹æµ‹é…ç½®"""
    base_url: str = "http://localhost:8000"
    total_requests: int = 100
    concurrency: int = 10
    timeout: float = 30.0


class RequestResult(NamedTuple):
    """è¯·æ±‚ç»“æœ"""
    success: bool
    latency: float  # ç§’
    status_code: int
    error: str | None = None


# æµ‹è¯•æŸ¥è¯¢åˆ—è¡¨
TEST_QUERIES = [
    "ä½ å¥½",
    "iPhone 15 Pro å¤šå°‘é’±ï¼Ÿ",
    "æŸ¥ä¸€ä¸‹æˆ‘çš„è®¢å•",
    "æˆ‘æƒ³é€€è´§",
    "åä¸º Mate 60 æœ‰ä»€ä¹ˆé¢œè‰²ï¼Ÿ",
    "å¿«é€’åˆ°å“ªäº†ï¼Ÿ",
    "ä¼šå‘˜æŠ˜æ‰£æ˜¯å¤šå°‘ï¼Ÿ",
    "æ€ä¹ˆç”³è¯·æ¢è´§ï¼Ÿ",
    "MacBook Pro é…ç½®æ€ä¹ˆæ ·ï¼Ÿ",
    "æœ‰ä»€ä¹ˆä¼˜æƒ æ´»åŠ¨ï¼Ÿ",
]


async def send_request(session: aiohttp.ClientSession, config: BenchmarkConfig, query: str) -> RequestResult:
    """å‘é€å•ä¸ªè¯·æ±‚"""
    start_time = time.perf_counter()
    
    try:
        async with session.post(
            f"{config.base_url}/chat",
            json={"message": query, "use_tools": False},
            timeout=aiohttp.ClientTimeout(total=config.timeout),
        ) as response:
            await response.json()
            latency = time.perf_counter() - start_time
            
            return RequestResult(
                success=response.status == 200,
                latency=latency,
                status_code=response.status,
            )
    except asyncio.TimeoutError:
        latency = time.perf_counter() - start_time
        return RequestResult(
            success=False,
            latency=latency,
            status_code=0,
            error="Timeout",
        )
    except Exception as e:
        latency = time.perf_counter() - start_time
        return RequestResult(
            success=False,
            latency=latency,
            status_code=0,
            error=str(e),
        )


async def run_benchmark(config: BenchmarkConfig) -> dict:
    """æ‰§è¡Œå‹æµ‹"""
    print(f"\nğŸš€ å¼€å§‹å‹æµ‹...")
    print(f"   ç›®æ ‡: {config.base_url}")
    print(f"   æ€»è¯·æ±‚æ•°: {config.total_requests}")
    print(f"   å¹¶å‘æ•°: {config.concurrency}")
    
    results: list[RequestResult] = []
    
    connector = aiohttp.TCPConnector(limit=config.concurrency)
    async with aiohttp.ClientSession(connector=connector) as session:
        # åˆ›å»ºä»»åŠ¡
        tasks = []
        for i in range(config.total_requests):
            query = TEST_QUERIES[i % len(TEST_QUERIES)]
            tasks.append(send_request(session, config, query))
        
        # æ‰§è¡Œå¹¶æ”¶é›†ç»“æœ
        start_time = time.perf_counter()
        results = await asyncio.gather(*tasks)
        total_time = time.perf_counter() - start_time
    
    # è®¡ç®—æŒ‡æ ‡
    latencies = [r.latency for r in results]
    success_count = sum(1 for r in results if r.success)
    
    metrics = {
        "config": {
            "base_url": config.base_url,
            "total_requests": config.total_requests,
            "concurrency": config.concurrency,
        },
        "summary": {
            "total_time": round(total_time, 2),
            "success_count": success_count,
            "failure_count": config.total_requests - success_count,
            "success_rate": round(success_count / config.total_requests, 4),
            "qps": round(config.total_requests / total_time, 2),
            "throughput": round(success_count / total_time, 2),
        },
        "latency": {
            "min": round(min(latencies), 3),
            "max": round(max(latencies), 3),
            "avg": round(statistics.mean(latencies), 3),
            "p50": round(statistics.median(latencies), 3),
            "p95": round(sorted(latencies)[int(len(latencies) * 0.95)], 3),
            "p99": round(sorted(latencies)[int(len(latencies) * 0.99)], 3),
        },
    }
    
    # é”™è¯¯ç»Ÿè®¡
    errors = [r.error for r in results if r.error]
    if errors:
        from collections import Counter
        metrics["errors"] = dict(Counter(errors))
    
    return metrics


def print_results(metrics: dict):
    """æ‰“å°å‹æµ‹ç»“æœ"""
    print("\n" + "=" * 60)
    print("ğŸ“Š æ€§èƒ½å‹æµ‹ç»“æœ")
    print("=" * 60)
    
    summary = metrics["summary"]
    latency = metrics["latency"]
    
    print(f"\nğŸ“ˆ æ±‡æ€»æŒ‡æ ‡:")
    print(f"   æ€»è€—æ—¶: {summary['total_time']}s")
    print(f"   æˆåŠŸ/å¤±è´¥: {summary['success_count']}/{summary['failure_count']}")
    print(f"   æˆåŠŸç‡: {summary['success_rate']:.1%}")
    print(f"   QPS: {summary['qps']}")
    print(f"   ååé‡: {summary['throughput']} req/s")
    
    print(f"\nâ±ï¸ å»¶è¿Ÿåˆ†å¸ƒ (ç§’):")
    print(f"   æœ€å°: {latency['min']}")
    print(f"   æœ€å¤§: {latency['max']}")
    print(f"   å¹³å‡: {latency['avg']}")
    print(f"   P50:  {latency['p50']}")
    print(f"   P95:  {latency['p95']}")
    print(f"   P99:  {latency['p99']}")
    
    if "errors" in metrics:
        print(f"\nâŒ é”™è¯¯ç»Ÿè®¡:")
        for error, count in metrics["errors"].items():
            print(f"   {error}: {count}")
    
    print("\n" + "=" * 60)
    
    # æ€§èƒ½è¯„ä¼°
    qps = summary["qps"]
    p95 = latency["p95"]
    
    if qps >= 10 and p95 < 3:
        print("ğŸ‰ ä¼˜ç§€ï¼QPS >= 10, P95 < 3s")
    elif qps >= 5 and p95 < 5:
        print("ğŸ‘ è‰¯å¥½ï¼å»ºè®®ä¼˜åŒ–å“åº”æ—¶é—´")
    else:
        print("âš ï¸ éœ€è¦ä¼˜åŒ–ï¼å¹¶å‘æ€§èƒ½è¾ƒä½")


def save_results(metrics: dict, output_path: Path):
    """ä¿å­˜å‹æµ‹ç»“æœ"""
    metrics["timestamp"] = datetime.now().isoformat()
    
    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(metrics, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜è‡³: {output_path}")


def main():
    import argparse
    
    parser = argparse.ArgumentParser(description="æ€§èƒ½å‹æµ‹è„šæœ¬")
    parser.add_argument("--url", default="http://localhost:8000", help="ç›®æ ‡ URL")
    parser.add_argument("--requests", "-n", type=int, default=50, help="æ€»è¯·æ±‚æ•°")
    parser.add_argument("--concurrency", "-c", type=int, default=5, help="å¹¶å‘æ•°")
    parser.add_argument("--timeout", "-t", type=float, default=30.0, help="è¶…æ—¶æ—¶é—´")
    args = parser.parse_args()
    
    config = BenchmarkConfig(
        base_url=args.url,
        total_requests=args.requests,
        concurrency=args.concurrency,
        timeout=args.timeout,
    )
    
    # æ‰§è¡Œå‹æµ‹
    metrics = asyncio.run(run_benchmark(config))
    
    # æ‰“å°ç»“æœ
    print_results(metrics)
    
    # ä¿å­˜ç»“æœ
    project_root = Path(__file__).parent.parent
    output_path = project_root / "data" / "eval" / "benchmark_results.json"
    save_results(metrics, output_path)


if __name__ == "__main__":
    main()
